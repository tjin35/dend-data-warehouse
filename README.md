# Data Warehouse Project

Building an ETL pipeline that extracts Sparkify's user and song data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for the analytics team.

## Dataset 

Working with two datasets that reside in S3:
```
s3://udacity-dend/song_data
```
The first dataset is a subset of real data from the  **[Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/)**. Each file is in JSON format and contains metadata about a song and the artist of that song. 

```
s3://udacity-dend/log_data
```
The second dataset consists of log files in JSON format generated by this **[event simulator](https://github.com/Interana/eventsim)** based on the songs in the dataset above. 


## Soluctions

Since the goal is to help analytics team to continue finding insights in what songs their users are listening to, create songplays table as the fact table and create users, artists, songs, time table to support the data analytics insights. The ETL pipleline loads song data and song play log from S3 to populate the staging tables staging_event and staging_songs, then using insert statements to load data into the final fact and dimension tables.


## Scripts

**create_tables.py** create the fact and dimension tables for the star schema in Redshift.

**sql_queries.py** define the SQL statements, which will be imported into the create_table.py and etl.py.

**etl.py** load data from S3 into staging tables on Redshift and then process that data into the analytics tables on Redshift.


## Running the tests

Run create_tables.py and follow by the etl.py in terminal to create talbes and load data.
# dend-data-lake
